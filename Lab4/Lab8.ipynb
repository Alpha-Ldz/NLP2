{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bertopic\n",
    "#!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7150ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34cf6a2",
   "metadata": {},
   "source": [
    "### Evaluating the dataset (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2dd714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2970\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2d3cd",
   "metadata": {},
   "source": [
    "The dataset is splitted into 3<br>\n",
    "train 9000<br>\n",
    "validation 1000<br> \n",
    "test 2970<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34409257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate entires in the train set is 0.42033333333333334\n",
      "Hate entires in the validation set is 0.427\n",
      "Hate entires in the test set is 0.42154882154882156\n"
     ]
    }
   ],
   "source": [
    "print(f\"hate entires in the train set is {sum(dataset['train']['label'])/len(dataset['train']['label'])}\")\n",
    "print(f\"Hate entires in the validation set is {sum(dataset['validation']['label'])/len(dataset['validation']['label'])}\")\n",
    "print(f\"Hate entires in the test set is {sum(dataset['test']['label'])/len(dataset['test']['label'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f1705",
   "metadata": {},
   "source": [
    "In all three datasets, there is the same proportion of hateful entries <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(random_state=42)\n",
    "model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", n_gram_range=(2,5), umap_model=umap_model)\n",
    "topics, probs = model.fit_transform(dataset['train']['text'])\n",
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7429c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(random_state=42)\n",
    "model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", n_gram_range=(2,5), umap_model=umap_model)\n",
    "topics, probs = model.fit_transform(list(map(lambda x : x['text'], filter(lambda x : x['label'] == 0, dataset['train']))))\n",
    "model.visualize_topics()\n",
    "model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "09abbbdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded\n",
      "['not-hate', 'hate']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "task='hate'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "'''try : \n",
    "    PATH = 'models/cased_L-12_H-768_A-12/'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PATH, local_files_only=True)\n",
    "    print('loaded locally')\n",
    "except :'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "print('downloaded')\n",
    "\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "print(labels)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210eb820",
   "metadata": {},
   "source": [
    "### Evaluate a model (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "070f382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.6538143762049022\n",
      "precision score : 0.4989491382934006\n",
      "recall score : 0.9480830670926518\n"
     ]
    }
   ],
   "source": [
    "task='hate'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "\n",
    "def pred(model, tokenizer, inputs):\n",
    "    preds = []\n",
    "    for text in inputs['text']:\n",
    "        #text = preprocess(text)\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        preds.append(np.argmax(scores))\n",
    "\n",
    "    return preds\n",
    "preds = pred(model, tokenizer, dataset['test'])\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(f\"F1 score : {f1_score(dataset['test']['label'], preds)}\")\n",
    "print(f\"precision score : {precision_score(dataset['test']['label'], preds)}\")\n",
    "print(f\"recall score : {recall_score(dataset['test']['label'], preds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819876d",
   "metadata": {},
   "source": [
    "### Theoritical questions (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e40c70",
   "metadata": {},
   "source": [
    "#### (2 point) What is the purpose of subword tokenization used by transformer models?\n",
    "#### Part of the answer is in the first part of the course (lesson 2).\n",
    "#### What is the effect on the vocabulary size?\n",
    "#### How does it impact out-of-vocabulary words (words which are not in the training data, but appear in the test data, or production environment)?\n",
    "\n",
    "The model is more comprehensive because it will be able to directly handle some tr√©s like plural, feminine / masculine etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44983d9",
   "metadata": {},
   "source": [
    "#### (2 point) When building an encoder-decoder model using an RNN, what is the purpose of adding attention?\n",
    "#### What problem are we trying to solve?\n",
    "#### How does attention solve the problem?\n",
    "\n",
    "One limitation of a classical RNN such as a seq2seq approach is that the RNN tries to remember the entire input sequence via a single hidden unit before translating it. Compressing all the information into a single hidden unit can result in information loss, especially for long sequences. Thus, similar to the way humans translate sentences, it may be beneficial to have access to the entire input sequence at each time step. \n",
    "\n",
    "Unlike an ordinary RNN, an attention mechanism allows the RNN to access all input elements at each given time step. However, having access to all elements of the input sequence at each time step can be overwhelming. Thus, to help the RNN focus on the most relevant elements of the input sequence, the attention mechanism assigns different attention weights to each input element. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbbc0c7",
   "metadata": {},
   "source": [
    "####  (2 point) In a transformer model what is the multihead attention used for?\n",
    "#### What are we trying to achieve with self-attention?\n",
    "#### Why do we use muliple head instead of one?\n",
    "\n",
    "Intuitively, multiple attention heads allow us to attend differently to certain parts of the sequence (e.g., long-term versus short-term dependencies). This allows us to reach a higher level of understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd34b66",
   "metadata": {},
   "source": [
    "####  (2 point) In a transformer model, what is the purpose of positional embedding?\n",
    "#### What would be the problem if we didn't use it?\n",
    "\n",
    "Embending is a way to represent the meaning and importance of a word in a sentence in a vector space in order to group and match words of similar or common meaning. Positional embending is a way to do this by including the position of the word in the sentence as a parameter which adds more meaning to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ca464",
   "metadata": {},
   "source": [
    "#### (2 point) What are the are the purpose of benchmarks?\n",
    "#### And are they reliable? Why?\n",
    "\n",
    "Benchmarks allow to compare different models and innovations to show the effect of technology changes. Their reliability is based on all other models already created so yes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d98f0",
   "metadata": {},
   "source": [
    "#### (4 points) What are the differences between BERT and GPT?\n",
    "#### What kind of transformer-based model are they?\n",
    "#### How are they pretrained?\n",
    "#### How are they fine-tuned?\n",
    "\n",
    "They are fundamentally different in that BERT has just the encoder blocks from the transformer, whilst GPT has just the decoder blocks from the transformer. \n",
    "\n",
    "BERT was pretrained on two tasks: language modeling (15% of tokens were masked and BERT was trained to predict them from context) and next sentence prediction (BERT was trained to predict if a chosen next sentence was probable or not given the first sentence). \n",
    "\n",
    "GPT use an unsupervised pre-trained\n",
    "\n",
    "BERT is fine-tuned by simply adding one additional layer after the final BERT layer and training the entire network for just a few epochs.\n",
    "\n",
    "Fine-tuning in GPT-3 is the process of adjusting the parameters of a pre-trained model to better suit a specific task. This can be done by providing GPT-3 with a data set that is tailored to the task at hand, or by manually adjusting the parameters of the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cfcd4",
   "metadata": {},
   "source": [
    "#### (2 points) How are zero-shot and few-shots learning different from fine-tuning?\n",
    "#### How do fine-tuning, zero-shot, and few-shot learning affect the model's weights?\n",
    "\n",
    "Rero-shot and few-shots make learn to the model of the features thanks to small specific dataset for this one where the fine-tuning touch directly the weight to the hand \n",
    "\n",
    "They work by forcing the model to learn on small datasets created to learn specific features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306cb4f",
   "metadata": {},
   "source": [
    "#### (2 point) In a few paragraphs, explain how the triplet loss is used to train a bi-encoder model for semantic similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1698f10",
   "metadata": {},
   "source": [
    "#### (2 point) What is the purpose of using an Approximate Nearest Neighbour method to speed up search?\n",
    "#### What does it really reduce?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
