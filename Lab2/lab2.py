# -*- coding: utf-8 -*-
"""Lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B4CM6M0Nq9XPhKwx_oatX4Dq9w_vvutc

# **NLP05**
"""

!pip install transformers[sentencepiece] datasets torch sklearn evaluate

from transformers import BertModel, AutoModelForSequenceClassification


import evaluate

"""# Import the dataset"""

import datasets

df = datasets.load_dataset("imdb")

"""# Tokenize"""

from transformers import AutoTokenizer, DataCollatorWithPadding

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

def tokenize_function(element: datasets.arrow_dataset) -> datasets.arrow_dataset:
  """
    Function take and element and tokenize it
    Args:
        - element : element selected
    Returns:
        the element tokenized
    """
  return tokenizer(element["text"], truncation=True)

tokenized_datasets = df.map(tokenize_function, batched=True)

"""# Split your test and validation dataset"""

from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(tokenized_datasets['train']["text"], tokenized_datasets['train']["label"], test_size=0.2)

"""# Generate the dataset of test and train"""

def create_dataset(features: list, labels: list) -> datasets.Dataset:
    """
    Function generate a formated dataset

    Args:
        - features (list): list of features
        - labels (list): list of labels
    Returns:
        A dataset with the right format.
    """

    union = zip(features, labels)
    dataset = []

    for feature, label in union: dataset.append({"text": feature, "label": label})

    return datasets.Dataset.from_list(dataset)

dataset_train = create_dataset(X_train, y_train)
dataset_validation = create_dataset(X_valid, y_valid)

token_train = dataset_train.map(tokenize_function, batched="True")
token_valid = dataset_validation.map(tokenize_function, batched="True")

"""# Prepare the training and the model"""

from transformers import TrainingArguments

#parameters for the train
training_args = TrainingArguments("test-trainer", num_train_epochs=1)

from transformers import AutoModelForSequenceClassification

#creation of the model
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

from transformers import Trainer

#Instancite de trainer
trainer = Trainer(
    model,
    training_args,
    train_dataset=token_train,
    eval_dataset=token_valid,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

"""# Evaluation

## Prediction
"""

predictions = trainer.predict(tokenized_datasets['test'])

"""## Get the accuracy"""

import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)

import evaluate

metric = evaluate.load('accuracy')
metric.compute(predictions=preds, references=predictions.label_ids)

"""# wrongly classified in the test set"""

import random

random.seed(42)

wrong = []
for i in range(len(tokenized_datasets['test']['label'])):
    if tokenized_datasets['test']['label'][i] != preds[i]:
      wrong.append((tokenized_datasets['test']['text'][i], tokenized_datasets['test']['label'][i]))
      if(len(wrong) == 2):
        break
wrong

"""# Question 4

**Question 4:** The advantages of Using BERT NLP Model is that it work well for task-specific models.
"""